{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daferocu/Topicos-Avanzados/blob/main/datalab/Talleres%202/E9%20-%20Question%20Answering/E9_QuestionAnswer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2NseOxJAIFM"
      },
      "source": [
        "# Question & Answer\n",
        "\n",
        "Creating a Question-Answer Transformer model or QA Transformer can be beneficial for several reasons, particularly in the field of Natural Language Processing (NLP). Here are some compelling reasons why you might want to develop a QA Transformer:\n",
        "\n",
        "1. **Question-Answering Systems:** QA Transformers are designed to provide accurate and contextually relevant answers to questions posed in natural language. These systems have a wide range of practical applications, including chatbots, virtual assistants, customer support, and information retrieval.\n",
        "\n",
        "2. **Information Retrieval:** QA Transformers can be used to search through large corpora of text and extract precise answers to user queries. This can improve the efficiency and effectiveness of information retrieval systems.\n",
        "\n",
        "3. **Document Summarization:** QA Transformers can be used to summarize long documents by answering questions about the document's content. This makes it easier for users to quickly understand the key points and relevant information in a text.\n",
        "\n",
        "4. **Education and E-Learning:** QA Transformers can be integrated into educational platforms to provide instant answers and explanations to students' questions. They can also help with the automatic generation of quiz questions and answers.\n",
        "\n",
        "5. **Content Generation:** QA Transformers can assist in content generation by automatically answering questions based on available knowledge. This can be useful for generating FAQs, product descriptions, and informative articles.\n",
        "\n",
        "6. **Customer Support:** Many companies use QA systems to automate responses to frequently asked questions, freeing up human agents to handle more complex queries and providing customers with quick solutions.\n",
        "\n",
        "7. **Medical Diagnosis:** QA Transformers can assist medical professionals by answering questions related to patient records, medical literature, and diagnostic information, potentially leading to faster and more accurate diagnoses.\n",
        "\n",
        "8. **Legal and Compliance:** In the legal field, QA Transformers can be used to search and extract information from legal documents, assisting lawyers in their research and case preparation.\n",
        "\n",
        "9. **Language Translation:** QA Transformers can be used to answer questions about language translation, helping users understand the meaning of words, phrases, or sentences in different languages.\n",
        "\n",
        "10. **Scientific Research:** QA Transformers can support researchers by answering questions related to scientific literature, allowing them to quickly access relevant information for their studies.\n",
        "\n",
        "11. **Decision Support:** QA Transformers can aid in decision-making processes by providing answers to questions related to data analysis, market research, and business intelligence.\n",
        "\n",
        "12. **Accessibility:** QA Transformers can improve accessibility for individuals with disabilities by providing spoken or written answers to their questions, helping them access information more easily.\n",
        "\n",
        "Overall, QA Transformers have the potential to enhance information retrieval, automation, and user interaction in various domains, making them a valuable tool in the development of intelligent systems and applications. The ability to provide accurate and context-aware answers to questions in natural language is a key advantage of these models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0voj5DTPAukU"
      },
      "source": [
        "---\n",
        "Exercise:\n",
        "\n",
        "Now, as a data scientist expert in NLP, you are asked to create a model to be able to answer question in Spanish. Your stakeholders will pass you an article and one question and your model should answer it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw-K8uOkAG95",
        "outputId": "19e7244a-f184-42b2-b363-842a276ac16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr29GPow1q_u",
        "outputId": "bc90149d-1e1a-45a2-b3b3-536833a6a3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Importamos funciones de NLTK para tokenizar el texto en oraciones y palabras\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Importamos la clase PorterStemmer de NLTK para hacer stemming (reducir palabras a su ra√≠z)\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Importamos TfidfVectorizer de scikit-learn para convertir el texto en vectores basados en TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Importamos la funci√≥n cosine_similarity de scikit-learn para medir la similitud entre vectores\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Importamos las stopwords de NLTK, que son palabras comunes que pueden ser ignoradas en an√°lisis de texto\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Importamos la librer√≠a string, que contiene funciones y constantes para manipular texto\n",
        "import string\n",
        "\n",
        "# Importamos requests para hacer peticiones HTTP y descargar contenido de sitios web\n",
        "import requests\n",
        "\n",
        "# Importamos BeautifulSoup de bs4 para analizar (parsear) el HTML y extraer informaci√≥n de p√°ginas web\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Importamos el tokenizador y el modelo de BERT para tareas de pregunta-respuesta\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "\n",
        "# Importamos torch para trabajar con tensores y manejar modelos de deep learning en PyTorch\n",
        "import torch\n",
        "\n",
        "# Importamos pipeline de transformers, que nos permite ejecutar tareas de NLP de manera sencilla\n",
        "from transformers import pipeline\n",
        "\n",
        "# Importamos pandas, una librer√≠a para manipular y analizar datos tabulares en estructuras como DataFrames\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraemos el contenido de texto que tiene la URL, usamos la solicitud HTTP get de la biblioteca requests para obtener el c√≥digo de la pagina. Si devuelve 200 quiere decir que la solicitud fue exitosa. Ahora con la biblioteca BeautifulSoup, que facilita la manipulacion dentro de los documentos HTML. El codigo busca una divisi√≥n con la clase \"article-content\", donde generalmente se encuentra el cuerpo del art√≠culo. A continuaci√≥n, extrae todo el texto dentro de las etiquetas \"p\" (que suelen representar p√°rrafos) y lo agrega a una cadena article_text."
      ],
      "metadata": {
        "id": "u0rhRM9nwZXq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQPl3Doa1q_u",
        "outputId": "aceed257-5887-4f31-95e7-3c78a53c8449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Over the course of February, Geoffrey Hinton, one of the most influential AI researchers of the past 50 years, had a ‚Äúslow eureka moment.‚Äù\n",
            "Hinton, 76, has spent his career trying to build AI systems that model the human brain, mostly in academia before joining Google in 2013. He had always believed that the brain was better than the machines that he and others were building, and that by making them more like the brain, they would improve. But in February, he realized ‚Äúthe digital intelligence we‚Äôve got now may be better than the brain already. It‚Äôs just not scaled up quite as big.‚Äù \n",
            "Developers around the world are currently racing to build the biggest AI systems that they can. Given the current rate at which AI companies are increasing the size of models, it could be less than five years until AI systems have 100 trillion connections‚Äîroughly as many as there are between neurons in the human brain.\n",
            "Alarmed, Hinton left his post as VP and engineering fellow in May and gave a flurry of interviews in which he explained that he had left in order to be able to speak freely on the dangers of AI‚Äîand his regrets over helping bring that technology into existence. He worries about what could happen once AI systems are scaled up to the size of human brains‚Äîand the prospect of humanity being wiped out by the technology he helped create. ‚ÄúThis stuff will get smarter than us and take over,‚Äù says Hinton. ‚ÄúAnd if you want to know what that feels like, ask a chicken.‚Äù\n",
            "Born and raised in England, Hinton comes from a long line of luminaries, with relatives including the mathematician Mary Everest Boole and logician George Boole, whose work is crucial to modern computer science; surgeon James Hinton; and surveyor George Everest, who gave his name to the mountain. \n",
            "The human brain always fascinated Hinton. As a Cambridge University undergraduate, he tried a range of subjects‚Äîphysiology, physics, philosophy‚Äîbefore graduating with a degree in experimental psychology in 1970. He worked briefly as a carpenter before starting a Ph.D. in AI at the University of Edinburgh, then the U.K.‚Äôs only postgraduate program on the subject, in 1972.\n",
            "In the 1970s, artificial intelligence, after failing to live up to its postwar promise, was going through a period of dampened enthusiasm now referred to as the ‚ÄúAI winter.‚Äù In this unfashionable field, Hinton pursued an unpopular idea: AI systems known as neural networks, which mimicked the structure of the human brain. His thesis adviser urged him on a weekly basis to change his approach. Each time he replied, ‚ÄúGive me another six months and I‚Äôll prove to you that it works.‚Äù\n",
            "Upon completion of his Ph.D., Hinton moved to the U.S., where more funding was available for his research. He published pathbreaking research, for which he was awarded the 2018 Turing Award, in posts at universities across the U.S., before eventually taking a professorship in computer science at the University of Toronto. Toronto has become Hinton‚Äôs home base; he travels relatively infrequently because back problems prevent him from sitting down. During car journeys he lies across the back seat; he eats kneeling before a table ‚Äúlike a monk at the altar‚Äù; and as he spoke to TIME he swayed gently in front of a head-height camera.\n",
            "In 2012, Hinton and two of his graduate students, Alex Krizhevsky and Ilya Sutskever, now chief scientist at OpenAI, entered ImageNet, a once annual competition in which researchers competed to build the most accurate image-recognition AI systems. They dominated the competition‚Äîan emphatic demonstration that neural networks had come of age. Hinton‚Äôs persistence had paid off.\n",
            "He and his two students began receiving lucrative offers from big tech companies. They set up a shell company called DNN-research to auction their expertise, and four tech firms‚ÄîGoogle, Microsoft, Baidu, and DeepMind‚Äîbid tens of millions for the company. After a week, Hinton chose Google over the final bidder, Baidu. In 2013, he joined Google Brain, the cutting-edge machine-learning team he left in May.\n",
            "Hinton has been instrumental in the development and popularization of neural networks, the dominant AI development paradigm that has allowed huge amounts of data to be ingested and processed, leading to advances in image recognition, language understanding, and self-driving cars. His work has potentially hastened the future he fears, in which AI becomes superhuman with disastrous results for humans. In an interview with the New York Times, Hinton said, ‚ÄúI console myself with the normal excuse: If I hadn‚Äôt done it, somebody else would have.‚Äù\n",
            "Hinton does not know how to prevent superhuman AI systems from taking over. If there‚Äôs any hope, he says, it lies with the next generation, noting that he feels too old to continue contributing to research. Many scientists switch to policy work later in their careers, but he declined Google‚Äôs offer to take such a role at the company. ‚ÄúI‚Äôve never been very good at or interested in policy issues,‚Äù he tells TIME. ‚ÄúI‚Äôm a scientist.‚Äù \n",
            "Instead, Hinton has spent the past few months sounding the alarm‚Äîhe can explain the technical details of AI in an accessible way as well as anyone and spends much of his time giving interviews to raise public awareness. He has also spoken with policymakers, including officials in the U.K. Prime Minister‚Äôs office, Canadian Prime Minister Justin Trudeau, Executive Vice-President of the European Commission Margrethe Vestager, and U.S. Senators Bernie Sanders and Jon Ossoff.\n",
            "While on a theoretical level he now grasps the risks from AI, Hinton says that his emotions haven‚Äôt yet caught up. ‚ÄúThe idea that we‚Äôre going to be replaced as the apex intelligence is just very hard to get your head around.‚Äù \n",
            "But for now, he takes his cues from another relative: his cousin Joan Hinton was one of the only women scientists to work on the Manhattan Project. After the nuclear weapons that she helped to create were dropped on Hiroshima and Nagasaki, she became a peace activist. In 1948 she moved to China, and she spent most of the rest of her life working on dairy farms as an ardent Maoist. Hinton‚Äôs own retirement plans are less strident but likewise bucolic: he intends to rediscover carpentry and take long walks.\n",
            "Write to Will Henshall at will.henshall@time.com.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# URL del art√≠culo\n",
        "url = \"https://time.com/collection/time100-ai/6309026/geoffrey-hinton/\"\n",
        "\n",
        "# Realizar una solicitud HTTP para obtener el contenido de la p√°gina\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificar si la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Analizar el contenido HTML de la p√°gina con BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Encontrar el contenido del art√≠culo\n",
        "    article_content = soup.find(\"div\", {\"class\": \"article-content\"})\n",
        "\n",
        "    # Extraer el texto del art√≠culo\n",
        "    article_text = \"\"\n",
        "    for paragraph in article_content.find_all(\"p\"):\n",
        "        article_text += paragraph.get_text() + \"\\n\"\n",
        "\n",
        "    # Imprimir el texto del art√≠culo\n",
        "    print(article_text)\n",
        "else:\n",
        "    print(\"Error al obtener la p√°gina:\", response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "StIrHeBAB07H"
      },
      "outputs": [],
      "source": [
        "question = \"How is Geoffrey Hinton?\""
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "0abq9zXPUFD1"
      },
      "source": [
        "# TFIDF con similitud"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos la implementaci√≥n un sistema b√°sico de TF-IDF (Term Frequency-Inverse Document Frequency) combinado con similitud de coseno para encontrar la oraci√≥n m√°s relevante de un texto en relaci√≥n a una pregunta dada. Primero, el texto (extra√≠do previamente) y la pregunta se tokenizan y preprocesan eliminando puntuaciones, palabras vac√≠as (stopwords) y aplicando stemming (reducci√≥n de palabras a su ra√≠z). Luego, se utiliza el vectorizador TF-IDF para crear una matriz que representa las oraciones del texto y la pregunta. Posteriormente, se calcula la similitud de coseno entre la pregunta y cada oraci√≥n del texto. La oraci√≥n con la mayor similitud es seleccionada como la respuesta a la pregunta."
      ],
      "metadata": {
        "id": "hloSkMdcyq6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XWm18Qoq1PGe"
      },
      "outputs": [],
      "source": [
        "# Define el texto y la pregunta\n",
        "texto = article_text\n",
        "question = \"How is Geoffrey Hinton?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d1OIPZn-1q_v"
      },
      "outputs": [],
      "source": [
        "# Tokeniza el texto en oraciones\n",
        "oraciones = sent_tokenize(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "og-g8yK12_RK"
      },
      "outputs": [],
      "source": [
        "# Funci√≥n de preprocesamiento\n",
        "def preprocesar(text):\n",
        "    # Tokeniza el texto en palabras\n",
        "    palabras = word_tokenize(text)\n",
        "\n",
        "    # Convierte todas las palabras a min√∫sculas y elimina los signos de puntuaci√≥n\n",
        "    palabras = [word.lower() for word in palabras if word not in string.punctuation]\n",
        "\n",
        "    # Elimina las palabras vac√≠as (stopwords) del texto\n",
        "    palabras = [word for word in palabras if word not in stopwords.words('english')]\n",
        "\n",
        "    # Inicializa el algoritmo de stemming de Porter, ayuda a normalizar palabras reduci√©ndolas a su forma base, como por ejemplo convertir ‚Äúrunning‚Äù y ‚Äúruns‚Äù en ‚Äúrun‚Äù.\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Aplica el stemming a cada palabra (reduce las palabras a su ra√≠z)\n",
        "    palabras = [stemmer.stem(word) for word in palabras]\n",
        "\n",
        "    # Une las palabras procesadas en una cadena de texto\n",
        "    return \" \".join(palabras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gYU8VV421q_v"
      },
      "outputs": [],
      "source": [
        "# Preprocesa todas las oraciones\n",
        "oraciones_preprocesadas = [preprocesar(oracion) for oracion in oraciones]\n",
        "# Preprocesa la pregunta\n",
        "pregunta_preprocesada = preprocesar(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NXS7odz01q_v"
      },
      "outputs": [],
      "source": [
        "# Crea la matriz TF-IDF para las oraciones y la pregunta\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(oraciones_preprocesadas + [pregunta_preprocesada])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8bG6T7fc1q_v"
      },
      "outputs": [],
      "source": [
        "# Calcula la similitud de coseno entre la pregunta y todas las oraciones\n",
        "similarity_scores = cosine_similarity(tfidf_matrix[-1:], tfidf_matrix[:-1]).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(similarity_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Auygd2PyFZs5",
        "outputId": "280ff783-992c-4748-dfd2-b1acd45962f5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.25227034 0.         0.         0.         0.         0.04120537\n",
            " 0.         0.08619847 0.06203478 0.11479562 0.         0.\n",
            " 0.03991913 0.         0.05394728 0.         0.06160146 0.\n",
            " 0.04403329 0.         0.13223232 0.         0.         0.08328896\n",
            " 0.         0.04042246 0.         0.09689021 0.         0.\n",
            " 0.         0.04407544 0.         0.         0.06863848 0.04646759\n",
            " 0.         0.         0.05832075 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T94COI-31q_v",
        "outputId": "e70c38f9-04de-48c5-e661-c039fec79d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pregunta: How is Geoffrey Hinton?\n",
            "Respuesta: Over the course of February, Geoffrey Hinton, one of the most influential AI researchers of the past 50 years, had a ‚Äúslow eureka moment.‚Äù\n",
            "Hinton, 76, has spent his career trying to build AI systems that model the human brain, mostly in academia before joining Google in 2013.\n"
          ]
        }
      ],
      "source": [
        "# Encuentra el √≠ndice de la oraci√≥n m√°s similar\n",
        "most_similar_sentence_index = similarity_scores.argmax()\n",
        "\n",
        "# Obtiene la oraci√≥n correspondiente como respuesta\n",
        "respuesta = oraciones[most_similar_sentence_index]\n",
        "print(\"Pregunta:\", question)\n",
        "print(\"Respuesta:\", respuesta)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "7ec-Au6B90cY"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizamos el modelo BERT preentrenado para la tarea de respuesta a preguntas. Incluimos el token de Hugging Face para acceder al modelo. Luego, carga el modelo BERT (bert-large-uncased-whole-word-masking-finetuned-squad) y su correspondiente tokenizador. A continuaci√≥n, se prepara la entrada para el modelo, combinando la pregunta y el texto en un formato adecuado y aplicando truncamiento si el texto excede los 512 tokens. Durante la inferencia, el modelo predice los logits de inicio y fin de la respuesta en el texto. Finalmente, se extraen los tokens correspondientes a la respuesta usando los √≠ndices predichos, y se decodifican para obtener la respuesta en formato legible."
      ],
      "metadata": {
        "id": "BEOEtoCkzQ8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "import getpass\n",
        "\n",
        "# Pedir el token al usuario\n",
        "HF_TOKEN = getpass.getpass('Introduce tu Hugging Face token:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-38qp_oc_2T",
        "outputId": "aa0c2795-8602-4024-a34b-49eb0287c722"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Introduce tu Hugging Face token:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g97__2F1q_v",
        "outputId": "90ea0db8-f896-4af0-907e-05c10ea16e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Cargar el modelo BERT y el tokenizador\n",
        "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQJTuB_v1q_v",
        "outputId": "22f7e205-cfec-4984-facf-af2a117f7020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        }
      ],
      "source": [
        "# Preparar la entrada para el modelo\n",
        "inputs = tokenizer.encode_plus(question, texto, add_special_tokens=True, return_tensors=\"pt\", truncation=True, max_length=512)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "snzQh1UO1q_v"
      },
      "outputs": [],
      "source": [
        "# Realizar la inferencia\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extraer los √≠ndices de inicio y fin de la respuesta\n",
        "answer_start = torch.argmax(outputs.start_logits)\n",
        "answer_end = torch.argmax(outputs.end_logits) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXwzvYgN1q_v",
        "outputId": "b9da633d-82c2-4141-967d-ad937e346120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta: one of the most influential ai researchers of the past 50 years\n"
          ]
        }
      ],
      "source": [
        "answer_tokens = inputs[\"input_ids\"][0][answer_start:answer_end]\n",
        "answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "print(\"Respuesta:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOxHQFyZKYD4"
      },
      "source": [
        "# Transformers Question Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta secci√≥n realizamos tareas de procesamiento utilizando pipelines de la biblioteca Transformers:\n",
        "\n",
        "\n",
        "\n",
        "1.   **Clasificaci√≥n de texto:** Se usa un pipeline de clasificaci√≥n de\n",
        "texto con el modelo distilbert-base-uncased-finetuned-sst-2-english. Se clasifica el art√≠culo (article_text) que ha sido truncado a 512 caracteres. El resultado de la clasificaci√≥n se muestra en un DataFrame para facilitar su an√°lisis.\n",
        "2.   **Respuesta a preguntas:** Se utiliza un pipeline de pregunta-respuesta (question-answering) con el modelo deepset/roberta-base-squad2, especificando el par√°metro clean_up_tokenization_spaces para evitar futuros problemas de tokenizaci√≥n. Se le hace una pregunta en espa√±ol sobre Geoffrey Hinton utilizando el contenido de article_text como contexto, y la respuesta se muestra tambi√©n en un DataFrame.\n",
        "3.   **Resumen de texto:** Se utiliza un pipeline de resumen con el modelo facebook/bart-large-cnn para generar un resumen del texto, limitando el resultado a 60 tokens. Se asegura de limpiar los espacios de tokenizaci√≥n durante el proceso y finalmente se imprime el resumen generado.\n"
      ],
      "metadata": {
        "id": "QBlrdXsB0jqS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "zvBBSvwCKbrB"
      },
      "outputs": [],
      "source": [
        "# Importa la funci√≥n 'pipeline' de la librer√≠a 'transformers'\n",
        "from transformers import pipeline\n",
        "\n",
        "# Crea un clasificador de texto usando el pipeline de 'text-classification'\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "kC-Ci-6OK2GX",
        "outputId": "4ec2cfcf-f31d-40fb-ba0d-397e519cde44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      label     score\n",
              "0  POSITIVE  0.787145"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf28a04c-e94c-4ceb-911f-f6be6086b307\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.787145</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf28a04c-e94c-4ceb-911f-f6be6086b307')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cf28a04c-e94c-4ceb-911f-f6be6086b307 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cf28a04c-e94c-4ceb-911f-f6be6086b307');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"POSITIVE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7871453762054443,\n        \"max\": 0.7871453762054443,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7871453762054443\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "article_text = article_text[:512]\n",
        "\n",
        "outputs = classifier(article_text)\n",
        "pd.DataFrame(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "s8iv_oFzLsR1",
        "outputId": "4d39f646-5f66-4ffb-fad5-5eff28d506a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      score  start  end              answer\n",
              "0  0.196659    118  136  slow eureka moment"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-54ba42e9-2170-49e5-a40c-32731719e797\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.196659</td>\n",
              "      <td>118</td>\n",
              "      <td>136</td>\n",
              "      <td>slow eureka moment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54ba42e9-2170-49e5-a40c-32731719e797')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-54ba42e9-2170-49e5-a40c-32731719e797 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-54ba42e9-2170-49e5-a40c-32731719e797');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.19665926694869995,\n        \"max\": 0.19665926694869995,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.19665926694869995\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 118,\n        \"max\": 118,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          118\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 136,\n        \"max\": 136,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          136\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"slow eureka moment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# Especificar el modelo para pregunta-respuesta y establecer el par√°metro 'clean_up_tokenization_spaces'\n",
        "reader = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"deepset/roberta-base-squad2\",\n",
        "    clean_up_tokenization_spaces=True\n",
        ")\n",
        "#pregunta en espa√±ol\n",
        "question = \"¬øQue ha hecho Geoffrey Hinton?\"\n",
        "outputs = reader(question=question, context=article_text)\n",
        "pd.DataFrame([outputs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz3oOqv8NYTK",
        "outputId": "89acd113-5342-4bb8-e379-75da29a7bab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Geoffrey Hinton, 76, has spent his career trying to build AI systems that model the human brain. He had always believed that the brain was better than the machines that he and others were building. But in February, he realized ‚Äúthe digital intelligence we‚Äôve\n"
          ]
        }
      ],
      "source": [
        "# Importa el pipeline de resumen y especifica el modelo que deseas usar para evitar usar el modelo por defecto\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", clean_up_tokenization_spaces=True)\n",
        "\n",
        "# Generar el resumen con un l√≠mite de longitud de 60 tokens y asegurarse de que se limpie correctamente la tokenizaci√≥n\n",
        "outputs = summarizer(article_text, max_length=60, clean_up_tokenization_spaces=True)\n",
        "\n",
        "# Imprimir el resumen generado\n",
        "print(outputs[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (article_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WWxbWajJOAI",
        "outputId": "2f685629-bf79-4eb5-8bbd-784f9692c04d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Over the course of February, Geoffrey Hinton, one of the most influential AI researchers of the past 50 years, had a ‚Äúslow eureka moment.‚Äù\n",
            "Hinton, 76, has spent his career trying to build AI systems that model the human brain, mostly in academia before joining Google in 2013. He had always believed that the brain was better than the machines that he and others were building, and that by making them more like the brain, they would improve. But in February, he realized ‚Äúthe digital intelligence we‚Äôve got now \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traducci√≥n texto con HugginFace"
      ],
      "metadata": {
        "id": "EwuzCFFUll3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este c√≥digo carga un modelo preentrenado de MarianMT para traducir texto del ingl√©s al espa√±ol. Utiliza el tokenizador para preparar el texto en ingl√©s y genera su traducci√≥n al espa√±ol mediante el modelo. Luego, decodifica los tokens traducidos y muestra el art√≠culo traducido en espa√±ol, para finalmente realizar la pregunta y respuesta en espa√±ol."
      ],
      "metadata": {
        "id": "jjIYBgJw1wkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# Cargar el modelo y el tokenizador para traducci√≥n ingl√©s-espa√±ol\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Funci√≥n para traducir texto del ingl√©s al espa√±ol\n",
        "def traducir(texto):\n",
        "    # Tokeniza el texto de entrada\n",
        "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=\"longest\")\n",
        "    # Genera la traducci√≥n\n",
        "    translated_tokens = model.generate(**inputs)\n",
        "    # Decodifica los tokens traducidos\n",
        "    traduccion = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
        "    return traduccion\n",
        "\n",
        "# Traducir el art√≠culo al espa√±ol\n",
        "articulo_traducido = traducir(article_text)\n",
        "\n",
        "print(\"Art√≠culo traducido:\", articulo_traducido)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PvyqZ58MeOL",
        "outputId": "9453a991-5b7a-4ebf-eb7e-56e87685c301"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Art√≠culo traducido: A lo largo de febrero, Geoffrey Hinton, uno de los investigadores de IA m√°s influyentes de los √∫ltimos 50 a√±os, tuvo un ‚Äúmomento eureka lento.‚Äù Hinton, de 76 a√±os, ha pasado su carrera tratando de construir sistemas de IA que modelen el cerebro humano, sobre todo en la academia antes de unirse a Google en 2013. Siempre hab√≠a cre√≠do que el cerebro era mejor que las m√°quinas que √©l y otros estaban construyendo, y que al hacerlos m√°s como el cerebro, mejorar√≠an. Pero en febrero, se dio cuenta de ‚Äúla inteligencia digital que tenemos ahora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creaci√≥n de un pipeline de respuesta a preguntas en espa√±ol\n",
        "reader = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "question = \"¬øQuien es Geoffrey Hinton?\"\n",
        "outputs = reader(question=question, context=articulo_traducido)\n",
        "\n",
        "# Mostrar la respuesta a la pregunta en espa√±ol\n",
        "print(\"Respuesta:\", outputs['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sJC_-Q_sERd",
        "outputId": "889585fa-c1c0-4e89-a559-0727146fbd36"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta: uno de los investigadores de IA m√°s influyentes\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}